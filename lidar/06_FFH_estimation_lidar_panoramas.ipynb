{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements FFH estimation using:\n",
    "* feature detection results from streetview images\n",
    "* elevation, depth and classification rasters of building facades derived from lidar data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "# from GSV.geometry import extract_feature_pixels_lowest_region, calculate_height_difference, calculate_width_difference, estimate_FFH, estimate_FFE\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.ndimage import label, generate_binary_structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set input parameters\n",
    "* building points containing ground elevation data\n",
    "* object detection results from streetview panoramas, containing buildnig ID and panorama ID\n",
    "* folder containing building facade elevation and classifcation rasters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "building_points_file=r'/home/ubuntu/lavender_floor_height/output/Final_Wagga_training_samples_pano_metadata_clipping_elevations.geojson'\n",
    "predictions_file=r'/home/ubuntu/lavender_floor_height/output/object_detection_results.csv' # example file for now\n",
    "las_projections_folder=r\"/mnt/floorheightvolume/lidar_Wagga/clipped_projected/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read feature detection results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = pd.read_csv(predictions_file)\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of building UFIs\n",
    "UFIs should be unique due to our processing is per building UIF based, while panorama IDs may not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pano_ids=df_predictions['pano_id'].unique().tolist() # subject to change depending on how object detection results are written"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in building points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_building_points=gpd.read_file(building_points_file)\n",
    "gdf_building_points=gdf_building_points[gdf_building_points[\"USAGE\"]==\"Residential\"].reset_index(drop=True)\n",
    "gdf_building_points.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_building_points.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test calculation with one example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_building_points[gdf_building_points['UFI']==2147]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=113\n",
    "ufi=gdf_building_points['UFI'][i]\n",
    "ufi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_single=df_predictions[df_predictions['UFI']==ufi]\n",
    "df_single=df_predictions[df_predictions['pano_id']=='xQyZzUB9aD6e9H-dBUJBfw'].reset_index(drop=True)\n",
    "df_single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # to deletd - synthetic data for testing\n",
    "df_single['x1'] = df_single['x1']-600\n",
    "df_single['x2'] = df_single['x2']-600\n",
    "df_single['y1'] = df_single['y1']-600\n",
    "df_single['y2'] = df_single['y2']-600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_elevation_gapfill = gdf_building_points.iloc[i]['lidar_elev_25pct']\n",
    "ground_elevation_gapfill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in corresponding projected rasters\n",
    "* Elevation\n",
    "* classification\n",
    "* Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elevation_file=glob.glob(os.path.join(las_projections_folder,'*'+'_UFI_'+str(ufi)+'_elevation_resampled.tif'))[0]\n",
    "classification_file=glob.glob(os.path.join(las_projections_folder,'*'+'_UFI_'+str(ufi)+'_classification_resampled.tif'))[0]\n",
    "depth_file=glob.glob(os.path.join(las_projections_folder,'*'+'_UFI_'+str(ufi)+'_depth_resampled.tif'))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_arr=np.array(Image.open(classification_file))\n",
    "elevation_arr=np.array(Image.open(elevation_file))\n",
    "depth_arr=np.array(Image.open(depth_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate gapfilling depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gapfill_depth(depth_arr, classification_arr,nodata_depth=9999):\n",
    "    # Create mask of all valid pixels in entire array\n",
    "    full_mask = depth_arr != nodata_depth\n",
    "    if not np.any(full_mask):\n",
    "        return nodata_depth  # no valid pixels in entire image\n",
    "    # create mask of building pixels\n",
    "    building_mask=(classification_arr==6)&(depth_arr != nodata_depth)\n",
    "    gapfill_depth=np.mean(depth_arr[building_mask])\n",
    "    return gapfill_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gapfill_depth=calculate_gapfill_depth(depth_arr, classification_arr,nodata_depth=9999)\n",
    "gapfill_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate feature properties\n",
    "* metrics for filtering: width, height, area and width/heigh ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_interpolate(x, y, arr, nodata=9999, search_radius=20):\n",
    "    \"\"\"Inverse-distance weighted interpolation using valid pixels within a radius.\n",
    "    If no valid pixels within radius, use the closest valid pixel value.\"\"\"\n",
    "    h, w = arr.shape\n",
    "    xi, yi = int(round(x)), int(round(y))\n",
    "    \n",
    "    # Define window bounds for initial search\n",
    "    x0, x1 = max(xi - search_radius, 0), min(xi + search_radius + 1, w)\n",
    "    y0, y1 = max(yi - search_radius, 0), min(yi + search_radius + 1, h)\n",
    "    window = arr[y0:y1, x0:x1]\n",
    "\n",
    "     # Create coordinate grids\n",
    "    yy, xx = np.meshgrid(np.arange(y0, y1), np.arange(x0, x1), indexing='ij')\n",
    "    mask = window != nodata\n",
    "    if not np.any(mask):\n",
    "        # Create mask of all valid pixels in entire array\n",
    "        full_mask = arr != nodata\n",
    "        if not np.any(full_mask):\n",
    "            return nodata  # no valid pixels in entire image\n",
    "        \n",
    "        # Find all valid pixel coordinates and values\n",
    "        valid_y, valid_x = np.where(full_mask)\n",
    "        valid_values = arr[valid_y, valid_x]\n",
    "        \n",
    "        # Calculate distances to all valid pixels\n",
    "        dists = np.sqrt((valid_x - x)**2 + (valid_y - y)**2)\n",
    "        \n",
    "        # Return the value of the closest pixel\n",
    "        closest_idx = np.argmin(dists)\n",
    "        return valid_values[closest_idx]\n",
    "    # Perform IDW interpolation with valid pixels in window\n",
    "    dists = np.sqrt((xx[mask] - x)**2 + (yy[mask] - y)**2)\n",
    "    values = window[mask]\n",
    "    weights = 1 / (dists + 1e-6)  # avoid divide-by-zero\n",
    "    return np.sum(weights * values) / np.sum(weights)\n",
    "\n",
    "def calculate_width_difference(left_pixel, right_pixel, depth_map, width_pano=11000, search_radius=20, nodata=9999):\n",
    "    \"\"\"Calculate real-world horizontal distance between two pixels using depth map\"\"\"\n",
    "\n",
    "    # extract depth\n",
    "    (xL, yL), (xR, yR) = left_pixel, right_pixel\n",
    "    depthL, depthR = depth_map[yL, xL], depth_map[yR, xR]\n",
    "    print('depths before depth interpolation ',depthL,depthR)\n",
    "\n",
    "    # interpolate depth values if invalid\n",
    "    depthL = extract_interpolate(x=xL, y=yL, arr=depth_map, nodata=nodata, search_radius=search_radius) if depthL == nodata else depthL\n",
    "    depthR = extract_interpolate(x=xR, y=yR, arr=depth_map, nodata=nodata, search_radius=search_radius) if depthR == nodata else depthR\n",
    "    print('depths after depth interpolation ',depthL,depthR)\n",
    "    if (depthL is None) or (depthR is None):\n",
    "        return None\n",
    "    \n",
    "    # calculate real world width\n",
    "    W_img = depth_map.shape[1]\n",
    "    angle_extend = W_img*180.0/width_pano\n",
    "    phi = lambda x: np.radians((2 * x / W_img - 1) * (angle_extend / 2))\n",
    "    print('phi ',phi)\n",
    "    x1, z1 = depthL * np.sin(phi(xL)), depthL * np.cos(phi(xL))\n",
    "    x2, z2 = depthR * np.sin(phi(xR)), depthR * np.cos(phi(xR))\n",
    "\n",
    "    return np.hypot(x2 - x1, z2 - z1)\n",
    "\n",
    "def calculate_height_difference(top_pixels, bottom_pixels, elevation_map, gapfill_depth,height_pano=5500, upper_crop=0.25, nodata=9999):\n",
    "    \"\"\"Calculate real-world horizontal distance between two pixels using elevation and depth maps\"\"\"\n",
    "    # extract elevations\n",
    "    (xT, yT), (xB, yB) = top_pixels, bottom_pixels\n",
    "    elevationT, elevationB = elevation_map[yT, xT], elevation_map[yB, xB]\n",
    "    # if invalid, use gapfilling depth and approximate\n",
    "    if (elevationT==nodata) or (elevationB==nodata):\n",
    "        print('using gapfilling depth')\n",
    "        # original y coordinate before cropping\n",
    "        yT_origin=yT+height_pano*upper_crop\n",
    "        theta_T = np.radians((height_pano/2.0-yT_origin)*(180.0/height_pano))\n",
    "        yB_origin=yB+height_pano*upper_crop\n",
    "        theta_B = np.radians((height_pano/2.0-yB_origin)*(180.0/height_pano))\n",
    "        return gapfill_depth*(np.sin(theta_T)-np.sin(theta_B))\n",
    "    height_difference=elevationT-elevationB\n",
    "    return height_difference\n",
    "\n",
    "def compute_feature_properties(row, elevation_arr, depth_arr, gapfill_depth, nodata=9999):\n",
    "    \"\"\"Calculate dimension metrics of detected features\"\"\"\n",
    "\n",
    "    # calculate average pixels of each boundary\n",
    "    mean_top, mean_bottom=(int(np.mean([row['x1'],row['x2']])),int(row['y1'])),(int(np.mean([row['x1'],row['x2']])),int(row['y2']))\n",
    "    mean_left, mean_right=(int(row['x1']),int(np.mean([row['y1'],row['y2']]))),(int(row['x2']),int(np.mean([row['y1'],row['y2']])))\n",
    "\n",
    "    # caculate feature top and bottom elevations\n",
    "    feature_top=elevation_arr[mean_top[1],mean_top[0]]\n",
    "    feature_bottom=elevation_arr[mean_bottom[1],mean_bottom[0]]\n",
    "    feature_top=None if feature_top==nodata else feature_top\n",
    "    feature_bottom=None if feature_bottom==nodata else feature_bottom\n",
    "    print('feature top elevation:',feature_top)\n",
    "    print('feature bottom elevation:', feature_bottom)\n",
    "\n",
    "    # calculate feature width\n",
    "    feature_width = calculate_width_difference(mean_left, mean_right,depth_map=depth_arr, width_pano=11000, nodata=nodata)\n",
    "    print('feature width',feature_width)\n",
    "\n",
    "    # calculate feature height\n",
    "    feature_height = calculate_height_difference(mean_top, mean_bottom, elevation_map=elevation_arr, gapfill_depth=gapfill_depth, height_pano=5500, nodata=nodata)\n",
    "    print('feature height',feature_height)\n",
    "\n",
    "    # calculate area and ratio\n",
    "    feature_area, dimension_ratio = None, None\n",
    "    if (feature_height is not None) and (feature_width is not None):\n",
    "        feature_area=feature_height*feature_width\n",
    "        dimension_ratio=feature_width/feature_height\n",
    "\n",
    "    return feature_top, feature_bottom, feature_width, feature_height, feature_area, dimension_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_single)):\n",
    "    df_single.loc[i,['top_elevation','bottom_elevation','width_m', 'height_m','area_m2','ratio']] = compute_feature_properties(row=df_single.iloc[i],elevation_arr=elevation_arr,depth_arr=depth_arr,gapfill_depth=gapfill_depth)\n",
    "df_single"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter detected features when multiple features are detected\n",
    "* Keep the front door with closest to standard feature metrics\n",
    "* Keep the lowest and most confident feature for the other classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define real-world target values for Australian front doors\n",
    "frontdoor_standards = {\n",
    "    \"width_m\": 0.82,      # meters\n",
    "    \"height_m\": 2.04,     # meters\n",
    "    \"area_m2\": 1.67,      # meters²\n",
    "    \"ratio\": 0.40         # width-to-height ratio\n",
    "}\n",
    "weights = {'area_m2': 1, 'ratio': 1, 'confidence':1, 'x_location':1, 'y_location': 1}  # Default weights\n",
    "classes=[\"foundation\", \"front door\", \"garage door\", \"stairs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_feature(df, weights, classes, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Select the best feature if multiples are detected\n",
    "    \"\"\"\n",
    "    selected_rows = []\n",
    "    for feature_class in classes:\n",
    "        subset = df[df['class'] == feature_class]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "        if feature_class in ['foundation', 'stairs', 'garage door']:\n",
    "            # Select bottom-most and with highest confidence\n",
    "            # best_row = subset.sort_values(by=['y2', 'confidence'], ascending=[False, False]).iloc[0]\n",
    "            weighted_diff = (\n",
    "                weights['area_m2'] * abs(subset['area_m2'] - frontdoor_standards['area_m2'])/frontdoor_standards['area_m2']+\n",
    "                weights['ratio'] * abs(subset['ratio'] - frontdoor_standards['ratio'])/frontdoor_standards['ratio']+\n",
    "                weights['confidence'] * subset['confidence']+\n",
    "                weights['y_location'] * subset['y2']/img_height\n",
    "            )\n",
    "            best_row = subset.iloc[np.argmin(weighted_diff)]\n",
    "        elif feature_class == 'front door':\n",
    "            # For front door: select closet to standard metrics, with highest confidence and horizontally closest to image centre\n",
    "            weighted_diff = (\n",
    "                weights['area_m2'] * abs(subset['area_m2'] - frontdoor_standards['area_m2'])/frontdoor_standards['area_m2']+\n",
    "                weights['ratio'] * abs(subset['ratio'] - frontdoor_standards['ratio'])/frontdoor_standards['ratio']+\n",
    "                weights['confidence'] * subset['confidence']+\n",
    "                weights['x_location'] * abs(img_width-(subset['x2']-subset['x1'])/2.0)/img_width\n",
    "                \n",
    "            )\n",
    "            best_row = subset.iloc[np.argmin(weighted_diff)]\n",
    "        selected_rows.append(best_row)\n",
    "    \n",
    "    return pd.DataFrame(selected_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_selected=select_best_feature(df_single, weights=weights, classes=classes, img_width=depth_arr.shape[1], img_height=depth_arr.shape[0]).reset_index(drop=True)\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate FFHs based on rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_closest_ground_to_feature(row, classification_arr, elevation_arr,min_area=5):\n",
    "    \"\"\"\n",
    "    Find the elevation of closest ground area to the average position of two points from the same feature.\n",
    "    \n",
    "    Args:\n",
    "        classification_arr: 2D array of classification values\n",
    "        elevation_arr: 2D array of elevation values\n",
    "        min_area: Minimum area threshold for ground regions\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with information about the closest ground area to the feature's average position,\n",
    "        or None if none found\n",
    "    \"\"\"\n",
    "    x1, y1 = int(row['x1']),int(row['y2'])\n",
    "    x2, y2 = int(row['x2']),int(row['y2'])\n",
    "\n",
    "    # Calculate average position of the two points\n",
    "    avg_y = int(round((y1 + y2) / 2))\n",
    "    avg_x = int(round((x1 + x2) / 2))\n",
    "\n",
    "    # Label all ground areas (search everywhere)\n",
    "    struct = generate_binary_structure(2, 2)\n",
    "    labeled_ground, _ = label((classification_arr == 2), structure=struct)\n",
    "\n",
    "    nearest_ground_elevation = None\n",
    "    min_distance = float('inf')\n",
    "\n",
    "    # Get unique labels in the array (excluding 0)\n",
    "    ground_labels = np.unique(labeled_ground)\n",
    "    ground_labels = ground_labels[ground_labels != 0]\n",
    "\n",
    "    for label_id in ground_labels:\n",
    "        ground_mask = (labeled_ground == label_id)\n",
    "        area_size = np.sum(ground_mask)\n",
    "        \n",
    "        if area_size >= min_area:\n",
    "            # Find closest point in this area to the average position\n",
    "            yy, xx = np.where(ground_mask)\n",
    "            distances = np.sqrt((yy - avg_y)**2 + (xx - avg_x)**2)\n",
    "            idx = np.argmin(distances)\n",
    "            current_dist = distances[idx]\n",
    "            \n",
    "            if current_dist < min_distance:\n",
    "                elev_values = elevation_arr[ground_mask]\n",
    "                nearest_ground_elevation=np.median(elev_values)\n",
    "                min_distance = current_dist\n",
    "\n",
    "    return nearest_ground_elevation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_closest_ground_to_feature(row=df_selected.iloc[0],classification_arr=classification_arr,elevation_arr=elevation_arr,min_area=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df_selected)):\n",
    "    df_selected.loc[i,['nearest_ground_elev']] = get_closest_ground_to_feature(row=df_selected.iloc[i],classification_arr=classification_arr,elevation_arr=elevation_arr,min_area=5)\n",
    "df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_FFH(df_features, max_ffh=2):\n",
    "    '''\n",
    "    Calculate FFHs using elevations of features and ground elevation values.\n",
    "    '''\n",
    "    # determine ground elevation: prioritisation garage door>stairs>foundation\n",
    "    elev_ground=None\n",
    "    df_features_filtered=df_features.dropna(subset=['bottom_elevation'])\n",
    "    filtered_classes=df_features_filtered['class'].values\n",
    "    print(filtered_classes)\n",
    "    if 'garage door' in filtered_classes:\n",
    "        elev_ground = df_features_filtered[df_features_filtered['class']=='garage door']['bottom_elevation'].values[0]\n",
    "    elif 'stairs' in filtered_classes:\n",
    "        elev_ground = df_features_filtered[df_features_filtered['class']=='stairs']['bottom_elevation'].values[0]\n",
    "    elif 'foundation' in filtered_classes:\n",
    "        elev_ground = df_features_filtered[df_features_filtered['class']=='foundation']['bottom_elevation'].values[0]\n",
    "    if elev_ground is None:\n",
    "        FFH_1=None\n",
    "    print('elev_ground',elev_ground)\n",
    "\n",
    "    # determine floor elevation: prioritisation front door>stairs>foundation\n",
    "    elev_floor=None\n",
    "    df_features_filtered=df_features.dropna(subset=['top_elevation'])\n",
    "    nearest_ground_elev=None\n",
    "    all_classes=df_features['class'].values\n",
    "    if 'front door' in all_classes:\n",
    "        frontdoor_bottom = df_features[df_features['class']=='front door']['bottom_elevation'].values[0]\n",
    "        if frontdoor_bottom is not None:\n",
    "            elev_floor = frontdoor_bottom\n",
    "            nearest_ground_elev=df_features[df_features['class']=='front door']['nearest_ground_elev'].values[0]\n",
    "    else:\n",
    "        df_features_filtered=df_features.dropna(subset=['top_elevation'])\n",
    "        if 'stairs' in filtered_classes:\n",
    "            elev_floor = df_features_filtered[df_features_filtered['class']=='stairs']['top_elevation'].values[0]\n",
    "            nearest_ground_elev=df_features[df_features['class']=='stairs']['nearest_ground_elev'].values[0]\n",
    "        elif 'foundation' in filtered_classes:\n",
    "            elev_floor = df_features_filtered[df_features_filtered['class']=='foundation']['top_elevation'].values[0]\n",
    "            nearest_ground_elev=df_features[df_features['class']=='foundation']['nearest_ground_elev'].values[0]\n",
    "    if elev_floor is None:\n",
    "        FFH_1=None\n",
    "    print('elev_floor',elev_floor)\n",
    "\n",
    "    # 1. FFH calculated from floor feature(front door/stair/foundation) and ground feature (stairs/foundation) - not always available\n",
    "    if (elev_floor is not None) and (elev_ground is not None):\n",
    "        FFH_1=elev_floor-elev_ground\n",
    "    \n",
    "    # 2. FFH calculated from floor feature (front door/stair/foundation) and ground elevation derived \n",
    "    # from closet ground area (likely available whenever a ground feature is detected)\n",
    "    FFH_2=None\n",
    "    if elev_floor is not None:\n",
    "        if nearest_ground_elev is not None:\n",
    "            FFH_2=elev_floor-nearest_ground_elev\n",
    "    \n",
    "    # 3. FFH calculated from floor feature (front door/stair/foundation) and ground elevation derived \n",
    "    # from DTM (almost always available whenever a ground feature is detected)\n",
    "    FFH_3 = None\n",
    "    if (elev_ground is not None) and (ground_elevation_gapfill is not None):\n",
    "        FFH_3 = elev_ground - ground_elevation_gapfill\n",
    "    return FFH_1, FFH_2, FFH_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch calculation for all buildings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
