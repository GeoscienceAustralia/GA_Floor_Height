{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference fine-tuned OneFormer on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForUniversalSegmentation\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = r\"D:\\Launceston\\GSV\\Pano_clipped\"\n",
    "model_folder = r\"D:\\Wagga\\RICS\\OneFormer\\from_all\"\n",
    "out_folder=r'D:\\Launceston\\GSV\\Panos_clipped_predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_folder = r\"D:\\Wagga\\RICS\\all_images\"\n",
    "# model_folder = r\"D:\\Wagga\\RICS\\OneFormer\\from_all\"\n",
    "# out_folder=r'D:\\Wagga\\RICS\\all_images_predicted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in GSV image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_files = sorted(glob(f\"{images_folder}/*.png\"))\n",
    "image_files = glob(f\"{images_folder}/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file=image_files[0]\n",
    "image = Image.open(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model and initialise processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2label =  {1:'front door',2:'foundation',3:'garage door',4:'pavement'}\n",
    "id2label = {0:\"_background_\", 1:\"foundation\", 2:\"front door\", 3:\"garage door\", 4:\"stairs\"}\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some kwargs in processor config are unused and will not have any effect: task_seq_length, max_seq_length. \n"
     ]
    }
   ],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_folder)\n",
    "# encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",size=(512,512))\n",
    "encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",do_resize=False)\n",
    "# processor.tokenizer.batch_decode(encoded_inputs.task_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at D:\\Wagga\\RICS\\OneFormer\\from_all were not used when initializing OneFormerForUniversalSegmentation: ['model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_projector.layers.1.0.weight']\n",
      "- This IS expected if you are initializing OneFormerForUniversalSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing OneFormerForUniversalSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneFormerForUniversalSegmentation(\n",
       "  (model): OneFormerModel(\n",
       "    (pixel_level_module): OneFormerPixelLevelModule(\n",
       "      (encoder): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 192, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): Identity()\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (1): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.013043479062616825)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.02608695812523365)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (1): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.03913043811917305)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.0521739162504673)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (1): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.06521739810705185)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (2): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.0782608762383461)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (3): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.09130435436964035)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (4): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1043478325009346)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (5): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.11739131063222885)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (6): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1304347962141037)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (7): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.14347827434539795)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (8): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.156521737575531)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (9): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.16956521570682526)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (10): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1826086938381195)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (11): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.19565218687057495)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (12): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.208695650100708)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (13): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.22173914313316345)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (14): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.2347826063632965)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (15): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.24782609939575195)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (16): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.260869562625885)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (17): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.27391305565834045)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=3072, out_features=1536, bias=False)\n",
       "                (norm): LayerNorm((3072,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.2869565188884735)\n",
       "                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (1): SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (key): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (value): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=1536, out_features=1536, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.30000001192092896)\n",
       "                  (layernorm_after): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=1536, out_features=6144, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=6144, out_features=1536, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): OneFormerPixelDecoder(\n",
       "        (position_embedding): OneFormerSinePositionEmbedding()\n",
       "        (input_projections): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder): OneFormerPixelDecoderEncoderOnly(\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x OneFormerPixelDecoderEncoderLayer(\n",
       "              (self_attn): OneFormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
       "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (adapter_1): Sequential(\n",
       "          (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): OneFormerTransformerModule(\n",
       "      (position_embedder): OneFormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(150, 256)\n",
       "      (decoder): OneFormerTransformerDecoder(\n",
       "        (query_transformer): OneFormerTransformerDecoderQueryTransformer(\n",
       "          (decoder): OneFormerTransformerDecoderQueryTransformerDecoder(\n",
       "            (layers): ModuleList(\n",
       "              (0-1): 2 x OneFormerTransformerDecoderQueryTransformerDecoderLayer(\n",
       "                (self_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (multihead_attn): MultiheadAttention(\n",
       "                  (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "                )\n",
       "                (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "                (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "                (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                (dropout1): Dropout(p=0.1, inplace=False)\n",
       "                (dropout2): Dropout(p=0.1, inplace=False)\n",
       "                (dropout3): Dropout(p=0.1, inplace=False)\n",
       "                (activation): ReLU()\n",
       "              )\n",
       "            )\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (decoder_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0-8): 9 x OneFormerTransformerDecoderLayer(\n",
       "            (cross_attn): OneFormerTransformerDecoderCrossAttentionLayer(\n",
       "              (multihead_attn): MultiheadAttention(\n",
       "                (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (self_attn): OneFormerTransformerDecoderSelfAttentionLayer(\n",
       "              (self_attn): OneFormerAttention(\n",
       "                (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "            (ffn): OneFormerTransformerDecoderFFNLayer(\n",
       "              (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation): ReLU()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (query_input_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (class_embed): Linear(in_features=256, out_features=6, bias=True)\n",
       "        (mask_embed): OneFormerMLPPredictionHead(\n",
       "          (layers): Sequential(\n",
       "            (0): PredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (1): PredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (2): PredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (level_embed): Embedding(3, 256)\n",
       "    )\n",
       "    (task_encoder): OneFormerTaskModel(\n",
       "      (task_mlp): OneFormerMLPPredictionHead(\n",
       "        (layers): Sequential(\n",
       "          (0): PredictionBlock(\n",
       "            (0): Linear(in_features=77, out_features=256, bias=True)\n",
       "            (1): ReLU()\n",
       "          )\n",
       "          (1): PredictionBlock(\n",
       "            (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (1): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (matcher): OneFormerHungarianMatcher()\n",
       "  (criterion): OneFormerLoss(\n",
       "    (matcher): OneFormerHungarianMatcher()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "device='cpu'\n",
    "model = AutoModelForUniversalSegmentation.from_pretrained(model_folder,is_training=False,\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        num_labels=len(label2id), \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "# model = AutoModelForUniversalSegmentation.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2867, 3640])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1],image.size[0])])[0]\n",
    "predicted_segmentation_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.fromarray(np.array(semantic_segmentation).astype(np.uint8))\n",
    "prediction_arr = Image.fromarray(np.array(predicted_segmentation_map).astype(np.uint8))\n",
    "\n",
    "# Save the image as a png\n",
    "out_prediction = os.path.join(out_folder,os.path.basename(image_file).replace('jpg','png'))\n",
    "prediction_arr.save(out_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together and do for all validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n",
      "prediction exists, skipping...\n"
     ]
    }
   ],
   "source": [
    "predictions=[]\n",
    "gt_labels=[]\n",
    "for image_file in image_files:\n",
    "    out_prediction = os.path.join(out_folder,os.path.basename(image_file).replace('jpg','png'))\n",
    "    if os.path.exists(out_prediction):\n",
    "        print('prediction exists, skipping...')\n",
    "    else:\n",
    "        image = Image.open(image_file)\n",
    "        # encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",size=(512,512))\n",
    "        encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",do_resize=False)\n",
    "        # processor.tokenizer.batch_decode(encoded_inputs.task_inputs)\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_inputs)\n",
    "        predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1],image.size[0])])[0]\n",
    "\n",
    "        # Save prediction image as a JPG\n",
    "        image_predicted = Image.fromarray(np.array(predicted_segmentation_map).astype(np.uint8))\n",
    "        image_predicted.save(out_prediction,compress_level=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
