{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, AutoModelForUniversalSegmentation, AutoProcessor, TrainingArguments, Trainer\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import labelme2coco\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import evaluate\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "from torch.optim import Adam\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pycocotools.coco import COCO\n",
    "from PIL import Image, ImageDraw\n",
    "import os\n",
    "from transformers import AutoProcessor  # Replace AutoProcessor with the specific processor if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, processor, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Directory with all the images.\n",
    "            processor: A Hugging Face processor to handle image and mask processing.\n",
    "            transform (callable, optional): Optional transform to be applied on the image and mask.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_dir = os.path.join(self.root_dir, \"images\")\n",
    "        self.ann_dir = os.path.join(self.root_dir, \"labels\")\n",
    "\n",
    "        # read images\n",
    "        image_file_names = []\n",
    "        for root, dirs, files in os.walk(self.img_dir):\n",
    "          image_file_names.extend(files)\n",
    "        self.images = sorted(image_file_names)\n",
    "\n",
    "        # read annotations\n",
    "        annotation_file_names = []\n",
    "        for root, dirs, files in os.walk(self.ann_dir):\n",
    "          annotation_file_names.extend(files)\n",
    "        self.annotations = sorted(annotation_file_names)\n",
    "\n",
    "        assert len(self.images) == len(self.annotations), \"There must be as many images as there are segmentation maps\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(os.path.join(self.img_dir, self.images[idx]))\n",
    "        segmentation_map = Image.open(os.path.join(self.ann_dir, self.annotations[idx]))\n",
    "        image=np.array(image)\n",
    "        segmentation_map = np.array(segmentation_map)\n",
    "        # Process images and masks using the processor\n",
    "        encoded_inputs = self.processor(\n",
    "            images=image,\n",
    "            segmentation_maps=segmentation_map,\n",
    "            task_inputs=[\"semantic\"],  # Specify task if using multi-task processors\n",
    "            return_tensors=\"pt\",\n",
    "            size=(512,512)\n",
    "        )\n",
    "\n",
    "        # encoded_inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in encoded_inputs.items()}\n",
    "        #for k,v in encoded_inputs.items():\n",
    "        #  encoded_inputs[k].squeeze_() # remove batch dimension\n",
    "        encoded_inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in encoded_inputs.items()}\n",
    "        return encoded_inputs\n",
    "\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        #A.Resize(256, 256),\n",
    "        A.ShiftScaleRotate(shift_limit=0.25, scale_limit=0.2, rotate_limit=20, p=0.5),\n",
    "        #A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.5),\n",
    "        #A.CLAHE (clip_limit=4.0, tile_grid_size=(8, 8), always_apply=False, p=0.5),\n",
    "        A.HueSaturationValue (hue_shift_limit=5, sat_shift_limit=20, val_shift_limit=20, p=0.5),\n",
    "        A.ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.3),\n",
    "        #A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        #ToTensorV2(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class COCOSegmentationDataset(Dataset):\n",
    "#     def __init__(self, root_dir, annotation_file, processor, transform=None):\n",
    "#         \"\"\"\n",
    "#         Args:\n",
    "#             root_dir (str): Directory with all the images.\n",
    "#             annotation_file (str): Path to the COCO annotation file.\n",
    "#             processor: A Hugging Face processor to handle image and mask processing.\n",
    "#             transform (callable, optional): Optional transform to be applied on the image and mask.\n",
    "#         \"\"\"\n",
    "#         self.root_dir = root_dir\n",
    "#         self.coco = COCO(annotation_file)\n",
    "#         self.ids = list(self.coco.imgs.keys())\n",
    "#         self.processor = processor\n",
    "#         self.transform = transform\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.ids)\n",
    "\n",
    "#     def __getitem__(self, index):\n",
    "#         # Load image and annotations\n",
    "#         img_id = self.ids[index]\n",
    "#         ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=False)\n",
    "#         anns = self.coco.loadAnns(ann_ids)\n",
    "#         img_info = self.coco.loadImgs(img_id)[0]\n",
    "        \n",
    "#         # Load image\n",
    "#         img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "#         print(img_info['file_name'])\n",
    "#         image = Image.open(img_info['file_name']).convert(\"RGB\")\n",
    "\n",
    "#         # Initialize an empty mask\n",
    "#         mask = Image.new('L', (img_info['width'], img_info['height']), 0)\n",
    "        \n",
    "\n",
    "#         # Draw each polygon annotation onto the mask\n",
    "#         for ann in anns:\n",
    "#             if 'segmentation' in ann:\n",
    "#                 category_id = ann[\"category_id\"]\n",
    "#                 for segmentation in ann['segmentation']:\n",
    "#                     polygon = [tuple(segmentation[i:i + 2]) for i in range(0, len(segmentation), 2)]\n",
    "#                     ImageDraw.Draw(mask).polygon(polygon, outline=category_id, fill=category_id)\n",
    "        \n",
    "#         # Convert mask to a tensor\n",
    "#         mask = torch.as_tensor(np.array(mask), dtype=torch.long)\n",
    "\n",
    "#         # Process images and masks using the processor\n",
    "#         encoded_inputs = self.processor(\n",
    "#             images=image,\n",
    "#             segmentation_maps=mask,\n",
    "#             task_inputs=[\"semantic\"],  # Specify task if using multi-task processors\n",
    "#             return_tensors=\"pt\",\n",
    "#             size=(512,512)\n",
    "#         )\n",
    "\n",
    "#         # Apply additional transformations if specified\n",
    "#         if self.transform:\n",
    "#             encoded_inputs[\"pixel_values\"] = self.transform(encoded_inputs[\"pixel_values\"])\n",
    "#             encoded_inputs[\"labels\"] = self.transform(encoded_inputs[\"labels\"])\n",
    "\n",
    "#         encoded_inputs = {k: v.squeeze() if isinstance(v, torch.Tensor) else v[0] for k,v in encoded_inputs.items()}\n",
    "\n",
    "#         return encoded_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained model and processror"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0:\"_background_\", 1:\"foundation\", 2:\"front door\", 3:\"garage door\", 4:\"stairs\"}\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_coco_swin_large and are newly initialized: ['model.text_mapper.prompt_ctx.weight', 'model.text_mapper.text_encoder.ln_final.bias', 'model.text_mapper.text_encoder.ln_final.weight', 'model.text_mapper.text_encoder.positional_embedding', 'model.text_mapper.text_encoder.token_embedding.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.0.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.0.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.1.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.1.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.2.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.2.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.3.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.3.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.4.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.4.self_attn.out_proj.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.layer_norm2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc1.weight', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.bias', 'model.text_mapper.text_encoder.transformer.layers.5.mlp.fc2.weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.in_proj_weight', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.bias', 'model.text_mapper.text_encoder.transformer.layers.5.self_attn.out_proj.weight', 'model.text_mapper.text_projector.layers.0.0.bias', 'model.text_mapper.text_projector.layers.0.0.weight', 'model.text_mapper.text_projector.layers.1.0.bias', 'model.text_mapper.text_projector.layers.1.0.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of OneFormerForUniversalSegmentation were not initialized from the model checkpoint at shi-labs/oneformer_coco_swin_large and are newly initialized because the shapes did not match:\n",
      "- model.transformer_module.decoder.class_embed.weight: found shape torch.Size([134, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.transformer_module.decoder.class_embed.bias: found shape torch.Size([134]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- criterion.empty_weight: found shape torch.Size([134]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"shi-labs/oneformer_coco_swin_large\"\n",
    "model = AutoModelForUniversalSegmentation.from_pretrained(\n",
    "    model_name,is_training=True,num_labels=len(id2label),\n",
    "    id2label=id2label,label2id=label2id,\n",
    "    ignore_mismatched_sizes=True)\n",
    "# move model to GPU\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "image_processor = AutoProcessor.from_pretrained(model_name)  # Replace with model ID\n",
    "image_processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the processor and model for OneFormer\n",
    "# model_name = \"shi-labs/oneformer_coco_swin_large\"\n",
    "# # model = AutoModelForSemanticSegmentation.from_pretrained(model_name,is_training=True)\n",
    "# model = AutoModelForUniversalSegmentation.from_pretrained(model_name,is_training=True,num_labels=4,ignore_mismatched_sizes=True)\n",
    "# processor = AutoProcessor.from_pretrained(model_name)  # Replace with model ID\n",
    "# processor.image_processor.num_text = model.config.num_queries - model.config.text_encoder_n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize dataset\n",
    "# root_dir = r'C:\\Users\\lliu\\FrontierSI\\Projects - 127 Residential Dwelling Floor Height\\4 Executing\\Data Exploration\\GSV\\Wagga\\Panos_clipped'\n",
    "# annotation_file = r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\Annotations_COCO\\dataset.json\"\n",
    "# train_dataset = COCOSegmentationDataset(root_dir, annotation_file, processor)\n",
    "# encoded_inputs = train_dataset[0]\n",
    "# encoded_inputs[\"pixel_values\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.imshow(np.array(encoded_inputs['mask_labels'][1,:,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoded_inputs[\"class_labels\"].shape\n",
    "# encoded_inputs[\"class_labels\"]\n",
    "# encoded_inputs[\"class_labels\"].squeeze().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a data collator\n",
    "def collate_fn(batch):\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in batch])\n",
    "    pixel_mask = torch.stack([example[\"pixel_mask\"] for example in batch])\n",
    "    class_labels = [example[\"class_labels\"] for example in batch]\n",
    "    mask_labels = [example[\"mask_labels\"] for example in batch]\n",
    "    #text_inputs = [example[\"text_inputs\"] for example in batch]\n",
    "    text_inputs = torch.stack([example[\"text_inputs\"] for example in batch])\n",
    "    #task_inputs = [example[\"task_inputs\"] for example in batch]\n",
    "    task_inputs = torch.stack([example[\"task_inputs\"] for example in batch])\n",
    "    return {\"pixel_values\": pixel_values, \"pixel_mask\": pixel_mask, \"class_labels\": class_labels, \"mask_labels\": mask_labels,'text_inputs':text_inputs,'task_inputs':task_inputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 245\n"
     ]
    }
   ],
   "source": [
    "# root_dir=r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_annotations_converted_merged\\train\"\n",
    "root_dir=r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_annotations_converted_merged\\train\"\n",
    "train_dataset = SemanticSegmentationDataset(root_dir=root_dir, processor=image_processor, transform=train_transform)\n",
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True,collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-0.2856, -0.2856, -0.2856,  ..., -0.4739, -0.4568, -0.4568],\n",
       "          [-0.2856, -0.2856, -0.2856,  ..., -0.4739, -0.4568, -0.4568],\n",
       "          [-0.2856, -0.2856, -0.2856,  ..., -0.4739, -0.4568, -0.4568],\n",
       "          ...,\n",
       "          [-0.4911, -0.5253, -0.4226,  ..., -0.8507, -0.6623, -0.9020],\n",
       "          [-0.7308, -0.5253, -0.4739,  ..., -0.9363, -0.8164, -0.6794],\n",
       "          [-0.4568, -0.4739, -0.4911,  ..., -0.8335, -0.7650, -0.6452]],\n",
       " \n",
       "         [[ 0.8880,  0.8880,  0.8880,  ...,  0.6604,  0.6779,  0.6779],\n",
       "          [ 0.8880,  0.8880,  0.8880,  ...,  0.6604,  0.6779,  0.6779],\n",
       "          [ 0.8880,  0.8880,  0.8880,  ...,  0.6604,  0.6779,  0.6779],\n",
       "          ...,\n",
       "          [-0.0749, -0.1099, -0.0049,  ..., -0.4076, -0.1975, -0.4251],\n",
       "          [-0.2500, -0.0749, -0.0049,  ..., -0.4601, -0.3725, -0.2500],\n",
       "          [ 0.0651,  0.0826,  0.0301,  ..., -0.4076, -0.3200, -0.1975]],\n",
       " \n",
       "         [[ 2.6051,  2.6051,  2.6051,  ...,  2.3088,  2.3263,  2.3263],\n",
       "          [ 2.6051,  2.6051,  2.6051,  ...,  2.3088,  2.3263,  2.3263],\n",
       "          [ 2.6051,  2.6051,  2.6051,  ...,  2.3088,  2.3263,  2.3263],\n",
       "          ...,\n",
       "          [ 0.5834,  0.4614,  0.4788,  ...,  0.2173,  0.3393,  0.1302],\n",
       "          [ 0.3219,  0.4962,  0.4962,  ...,  0.0431,  0.1999,  0.3219],\n",
       "          [ 0.5659,  0.6356,  0.5659,  ...,  0.1302,  0.3045,  0.4962]]]),\n",
       " 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1],\n",
       "         [1, 1, 1,  ..., 1, 1, 1]]),\n",
       " 'mask_labels': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "         [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]),\n",
       " 'class_labels': tensor([0, 1, 2]),\n",
       " 'text_inputs': tensor([[49406,   320,  1125,  ...,     0,     0,     0],\n",
       "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
       "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [49406,   320, 29119,  ...,     0,     0,     0],\n",
       "         [49406,   320, 29119,  ...,     0,     0,     0],\n",
       "         [49406,   320, 29119,  ...,     0,     0,     0]]),\n",
       " 'task_inputs': tensor([49406,   518, 10549,   533, 29119,  1550, 49407,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0])}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values tensor([[[[-0.3883, -0.1828, -0.3712,  ..., -0.2684, -0.3027, -0.5253],\n",
      "          [ 0.3652, -0.1999, -0.0458,  ...,  0.0912, -0.0629, -0.2684],\n",
      "          [ 1.0673, -0.4226, -0.1143,  ...,  0.7591,  0.7077,  0.1426],\n",
      "          ...,\n",
      "          [ 0.5022,  0.5022,  0.4851,  ...,  0.1254,  0.2282,  0.2967],\n",
      "          [ 0.5022,  0.4851,  0.4851,  ..., -0.1143, -0.0458, -0.0116],\n",
      "          [ 0.5022,  0.4851,  0.4851,  ..., -0.2513, -0.2171, -0.2171]],\n",
      "\n",
      "         [[-0.2325,  0.0126, -0.1625,  ...,  0.0826,  0.1352, -0.1450],\n",
      "          [ 0.5553,  0.0301,  0.2752,  ...,  0.4503,  0.3277,  0.0651],\n",
      "          [ 1.3431, -0.1975,  0.2227,  ...,  1.1681,  1.0980,  0.4678],\n",
      "          ...,\n",
      "          [ 0.9755,  0.9755,  0.9755,  ...,  0.2402,  0.3627,  0.4503],\n",
      "          [ 0.9755,  0.9580,  0.9755,  ..., -0.0049,  0.0826,  0.1352],\n",
      "          [ 0.9755,  0.9755,  0.9755,  ..., -0.1450, -0.0924, -0.0749]],\n",
      "\n",
      "         [[ 0.0082,  0.0256, -0.0964,  ...,  1.0017,  0.9668,  0.3916],\n",
      "          [ 1.2980,  0.4614,  0.2173,  ...,  1.3851,  1.1237,  0.6356],\n",
      "          [ 1.9603,  0.6182,  0.6182,  ...,  1.9777,  1.7163,  0.8971],\n",
      "          ...,\n",
      "          [ 1.7163,  1.7163,  1.6814,  ...,  0.3916,  0.4962,  0.5834],\n",
      "          [ 1.7163,  1.6988,  1.6814,  ...,  0.1476,  0.2173,  0.2696],\n",
      "          [ 1.7163,  1.6988,  1.6988,  ..., -0.0267,  0.0431,  0.0605]]]])\n",
      "pixel_mask tensor([[[1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         ...,\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1],\n",
      "         [1, 1, 1,  ..., 1, 1, 1]]])\n",
      "class_labels [tensor([0, 1, 2])]\n",
      "mask_labels [tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         ...,\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "         [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])]\n",
      "text_inputs tensor([[[49406,   320,  1125,  ...,     0,     0,     0],\n",
      "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
      "         [49406,   320,  1125,  ...,     0,     0,     0],\n",
      "         ...,\n",
      "         [49406,   320, 29119,  ...,     0,     0,     0],\n",
      "         [49406,   320, 29119,  ...,     0,     0,     0],\n",
      "         [49406,   320, 29119,  ...,     0,     0,     0]]])\n",
      "task_inputs tensor([[49406,   518, 10549,   533, 29119,  1550, 49407,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "for k,v in batch.items():\n",
    "  print(k,  v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65dee62c61344b29b9b8591d18b76a88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lliu\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "c:\\Users\\lliu\\miniconda3\\envs\\transformers\\lib\\site-packages\\transformers\\models\\oneformer\\modeling_oneformer.py:330: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast(enabled=False):\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     32\u001b[0m     num_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n\u001b[1;32m---> 33\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# evaluate\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lliu\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lliu\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\lliu\\miniconda3\\envs\\transformers\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import torchmetrics\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "# define optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "\n",
    "model.train()\n",
    "max_iou=0\n",
    "min_loss=10000\n",
    "for epoch in range(100):\n",
    "    print(\"Epoch:\", epoch)\n",
    "    running_loss = 0.0\n",
    "    num_samples = 0\n",
    "    #for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "    for idx, batch in enumerate(tqdm(train_loader)):\n",
    "        # get the inputs;\n",
    "        #pixel_values = batch[\"pixel_values\"].to(device)\n",
    "        #labels = batch[\"labels\"].to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model( pixel_values=batch[\"pixel_values\"].to(device),\n",
    "            pixel_mask =batch[\"pixel_mask\"].to(device),\n",
    "            mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "            class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "            #text_inputs=[labels.to(device) for labels in batch[\"text_inputs\"]],\n",
    "            text_inputs=batch[\"text_inputs\"].to(device),\n",
    "            #task_inputs=[labels.to(device) for labels in batch[\"task_inputs\"]],\n",
    "            task_inputs=batch[\"task_inputs\"].to(device),\n",
    "            )\n",
    "        loss = outputs.loss\n",
    "        batch_size = batch[\"pixel_values\"].size(0)\n",
    "        running_loss += loss.item()\n",
    "        num_samples += batch_size\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # evaluate\n",
    "    print(\"Loss:\", running_loss/num_samples)\n",
    "    if loss.item()<min_loss:\n",
    "        min_loss=loss.item()\n",
    "        model.save_pretrained(r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\segformer\\class_merged\\V3\")\n",
    "        image_processor.save_pretrained(r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\segformer\\class_merged\\V3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = evaluate.load(\"mean_iou\")\n",
    "# # import torchmetrics\n",
    "# # define optimizer\n",
    "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.00006)\n",
    "\n",
    "# # jaccard = torchmetrics.JaccardIndex(task=\"multiclass\", num_classes=4)\n",
    "# model.train()\n",
    "# max_iou=0\n",
    "# for epoch in range(100):  \n",
    "#     print(\"Epoch:\", epoch)\n",
    "#     running_loss = 0.0\n",
    "#     num_samples = 0\n",
    "#     #for idx, batch in enumerate(tqdm(train_dataloader)):\n",
    "#     for idx, batch in enumerate(train_loader):\n",
    "#         # get the inputs;\n",
    "#         # zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "#         #batch = {k:v.to(device) for k,v in batch.items()}\n",
    "#         # forward pass\n",
    "#         #outputs = model(**batch)\n",
    "#         # forward + backward + optimize\n",
    "#         outputs = model( pixel_values=batch[\"pixel_values\"].to(device),\n",
    "#             pixel_mask =batch[\"pixel_mask\"].to(device),\n",
    "#             mask_labels=[labels.to(device) for labels in batch[\"mask_labels\"]],\n",
    "#             class_labels=[labels.to(device) for labels in batch[\"class_labels\"]],\n",
    "#             #text_inputs=[labels.to(device) for labels in batch[\"text_inputs\"]],\n",
    "#             text_inputs=batch[\"text_inputs\"].to(device),\n",
    "#             #task_inputs=[labels.to(device) for labels in batch[\"task_inputs\"]],\n",
    "#             task_inputs=batch[\"task_inputs\"].to(device),\n",
    "\n",
    "#             )\n",
    "#         loss = outputs.loss\n",
    "#         batch_size = batch[\"pixel_values\"].size(0)\n",
    "#         running_loss += loss.item()\n",
    "#         num_samples += batch_size\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     # mean_iou_epoch=jaccard.compute()\n",
    "#     print(\"Loss:\", running_loss/num_samples)\n",
    "#     # print(\"Mean_IOU:\", mean_iou_epoch)\n",
    "#     # jaccard.reset()\n",
    "#     # if mean_iou_epoch>max_iou:\n",
    "#     #     max_iou=mean_iou_epoch\n",
    "#     # if epoch%10==0:\n",
    "#     #torch.save(model.state_dict(), r'X:\\50243-23WestgoldCue\\06_Output/'+str(epoch)+\".pth\")\n",
    "#     model.save_pretrained(r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\oneformer\\GPU\")\n",
    "#     processor.save_pretrained(r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\oneformer\\GPU\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
