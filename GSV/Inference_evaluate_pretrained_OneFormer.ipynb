{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference fine-tuned OneFormer on validation dataset and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForUniversalSegmentation\n",
    "import evaluate\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_folder = r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_annotations_converted_merged\\all\\images\"\n",
    "# labels_folder = r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_annotations_converted_merged\\all\\labels\"\n",
    "# model_folder = r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\oneformer\\from_all\"\n",
    "# out_folder=r'C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_prediction\\OneFormer\\from_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = r\"D:\\Wagga\\RICS\\annotations_converted_merged\\validation\\images\"\n",
    "labels_folder = r\"D:\\Wagga\\RICS\\annotations_converted_merged\\validation\\labels\"\n",
    "model_folder = r\"D:\\Wagga\\RICS\\OneFormer\\from_train_set\"\n",
    "out_folder=r'D:\\Wagga\\RICS\\OneFormer\\from_train_set\\prediction_validation_set'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in GSV image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = sorted(glob(f\"{images_folder}/*.png\"))\n",
    "label_files = sorted(glob(f\"{labels_folder}/*.png\"))\n",
    "assert len(image_files) == len(label_files), \"Number of images and labels do not match!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_file=image_files[0]\n",
    "image = Image.open(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model and initialise processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2label =  {1:'front door',2:'foundation',3:'garage door',4:'pavement'}\n",
    "id2label = {0:\"_background_\", 1:\"foundation\", 2:\"front door\", 3:\"garage door\", 4:\"stairs\"}\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_folder)\n",
    "encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",size=(512,512))\n",
    "processor.tokenizer.batch_decode(encoded_inputs.task_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "device='cpu'\n",
    "model = AutoModelForUniversalSegmentation.from_pretrained(model_folder,is_training=False,\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        num_labels=len(label2id), \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "# model = AutoModelForUniversalSegmentation.from_pretrained(model_path)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1],image.size[0])])[0]\n",
    "predicted_segmentation_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_gt=label_files[0]\n",
    "label_gt_map = Image.open(label_gt)\n",
    "# convert map to NumPy array\n",
    "label_gt_map = np.array(label_gt_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric expects a list of numpy arrays for both predictions and references\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "metrics = metric._compute(\n",
    "                  predictions=[predicted_segmentation_map],\n",
    "                  references=[label_gt_map],\n",
    "                  num_labels=len(id2label),\n",
    "                  ignore_index=255,\n",
    "                  reduce_labels=False, # we've already reduced the labels ourselves\n",
    "              )\n",
    "metrics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print overall metrics\n",
    "for key in list(metrics.keys())[:3]:\n",
    "  print(key, metrics[key])\n",
    "\n",
    "# pretty-print per category metrics as Pandas DataFrame\n",
    "metric_table = dict()\n",
    "for id, label in id2label.items():\n",
    "    metric_table[label] = [\n",
    "                           metrics[\"per_category_iou\"][id],\n",
    "                           metrics[\"per_category_accuracy\"][id]\n",
    "    ]\n",
    "\n",
    "print(\"---------------------\")\n",
    "print(\"per-category metrics:\")\n",
    "pd.DataFrame.from_dict(metric_table, orient=\"index\", columns=[\"IoU\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.fromarray(np.array(semantic_segmentation).astype(np.uint8))\n",
    "image = Image.fromarray(np.array(predicted_segmentation_map).astype(np.uint8))\n",
    "\n",
    "# Save the image as a JPG\n",
    "out_prediction = os.path.join(out_folder,os.path.basename(label_gt).replace('jpg','png'))\n",
    "image.save(out_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together and do for all validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "gt_labels=[]\n",
    "for image_file, label_gt in zip(image_files, label_files):\n",
    "\n",
    "    assert os.path.splitext(os.path.basename(image_file))[0] == os.path.splitext(os.path.basename(label_gt))[0], \\\n",
    "    f\"Image file '{image_file}' and label file '{label_gt}' do not match!\"\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",size=(512,512))\n",
    "    processor.tokenizer.batch_decode(encoded_inputs.task_inputs)\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encoded_inputs)\n",
    "    predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1],image.size[0])])[0]\n",
    "\n",
    "    # Save prediction image as a JPG\n",
    "    image_predicted = Image.fromarray(np.array(predicted_segmentation_map).astype(np.uint8))\n",
    "    out_prediction = os.path.join(out_folder,os.path.basename(label_gt).replace('jpg','png'))\n",
    "    image_predicted.save(out_prediction)\n",
    "\n",
    "    label_gt_map = Image.open(label_gt)\n",
    "    # convert map to NumPy array\n",
    "    label_gt_map = np.array(label_gt_map)\n",
    "\n",
    "    predictions.append(predicted_segmentation_map.flatten())\n",
    "    gt_labels.append(label_gt_map.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate - IoU by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric expects a list of numpy arrays for both predictions and references\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "metrics = metric._compute(\n",
    "                predictions=predictions,\n",
    "                references=gt_labels,\n",
    "                num_labels=len(id2label),\n",
    "                ignore_index=255,\n",
    "                reduce_labels=False, # we've already reduced the labels ourselves\n",
    "            )\n",
    "\n",
    "# pretty-print per category metrics as Pandas DataFrame\n",
    "metric_table = dict()\n",
    "for id, label in id2label.items():\n",
    "    metric_table[label] = [\n",
    "                        metrics[\"per_category_iou\"][id],\n",
    "                        metrics[\"per_category_accuracy\"][id]\n",
    "    ]\n",
    "\n",
    "print(\"Mean IoU:\",metrics[\"mean_iou\"])\n",
    "print(\"Mean accuracy:\",metrics[\"mean_accuracy\"])\n",
    "print(\"---------------------\")\n",
    "print(\"per-category metrics:\")\n",
    "pd.DataFrame.from_dict(metric_table, orient=\"index\", columns=[\"IoU\", \"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate - percent of samples with IoU>50%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute IoU and filter for IoU >= 50% for each sample\n",
    "ious_per_sample = []\n",
    "for i in range(len(gt_labels)):\n",
    "    results = metric._compute(references=[gt_labels[i]], predictions=[predictions[i]], num_labels=len(id2label),ignore_index=255,)\n",
    "    ious_per_sample.append(results['per_category_iou'])\n",
    "\n",
    "# Calculate percentage of IoU values higher than a threshold (e.g., 50%) for each sample\n",
    "iou_threshold = 0.2\n",
    "ious_per_category = np.array(ious_per_sample)\n",
    "percent_iou_above_threshold_per_category = (ious_per_category >= iou_threshold).sum(axis=0) / len(gt_labels) * 100\n",
    "percent_iou_above_threshold_per_category"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
