{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference fine-tuned OneFormer on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import AutoProcessor\n",
    "from transformers import AutoModelForUniversalSegmentation\n",
    "import evaluate\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images_folder = r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_annotations_converted_merged\\all\\images\"\n",
    "# model_folder = r\"C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\oneformer\\from_all\"\n",
    "# out_folder=r'C:\\Users\\lliu\\Desktop\\FrontierSI\\projects\\GA_floor_height\\GA-floor-height\\output\\Wagga\\GSV_prediction\\OneFormer\\from_all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_folder = r\"D:\\Wagga\\RICS\\all_images\"\n",
    "model_folder = r\"D:\\Wagga\\RICS\\OneFormer\\from_all\"\n",
    "out_folder=r'D:\\Wagga\\RICS\\all_images_predicted'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in GSV image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_files = sorted(glob(f\"{images_folder}/*.png\"))\n",
    "image_files = glob(f\"{images_folder}/*.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_file=image_files[0]\n",
    "image_file = r'D:\\Wagga\\RICS\\all_images\\Industrial_131112_04706_L_0007862.jpg'\n",
    "image = Image.open(image_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained model and initialise processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2label =  {1:'front door',2:'foundation',3:'garage door',4:'pavement'}\n",
    "id2label = {0:\"_background_\", 1:\"foundation\", 2:\"front door\", 3:\"garage door\", 4:\"stairs\"}\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = AutoProcessor.from_pretrained(model_folder)\n",
    "# encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",size=(512,512))\n",
    "encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",do_resize=False)\n",
    "# processor.tokenizer.batch_decode(encoded_inputs.task_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "device='cpu'\n",
    "model = AutoModelForUniversalSegmentation.from_pretrained(model_folder,is_training=False,\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        num_labels=len(label2id), \n",
    "                                                        id2label=id2label, \n",
    "                                                        label2id=label2id)\n",
    "# model = AutoModelForUniversalSegmentation.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "with torch.no_grad():\n",
    "  outputs = model(**encoded_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1],image.size[0])])[0]\n",
    "predicted_segmentation_map.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = Image.fromarray(np.array(semantic_segmentation).astype(np.uint8))\n",
    "prediction_arr = Image.fromarray(np.array(predicted_segmentation_map).astype(np.uint8))\n",
    "\n",
    "# Save the image as a JPG\n",
    "out_prediction = os.path.join(out_folder,os.path.basename(image_file).replace('jpg','png'))\n",
    "prediction_arr.save(out_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Put together and do for all validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "gt_labels=[]\n",
    "for image_file in image_files:\n",
    "    out_prediction = os.path.join(out_folder,os.path.basename(image_file).replace('jpg','png'))\n",
    "    if os.path.exists(out_prediction):\n",
    "        print('prediction exists, skipping...')\n",
    "    else:\n",
    "        image = Image.open(image_file)\n",
    "        # encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",size=(512,512))\n",
    "        encoded_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\",do_resize=False)\n",
    "        # processor.tokenizer.batch_decode(encoded_inputs.task_inputs)\n",
    "        # forward pass\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**encoded_inputs)\n",
    "        predicted_segmentation_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[(image.size[1],image.size[0])])[0]\n",
    "\n",
    "        # Save prediction image as a JPG\n",
    "        image_predicted = Image.fromarray(np.array(predicted_segmentation_map).astype(np.uint8))\n",
    "        image_predicted.save(out_prediction,compress_level=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
